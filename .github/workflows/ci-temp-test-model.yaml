name: ci-temp-test-model

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

# env:
#   CUDA_VISIBLE_DEVICES: 7

jobs:
  ci-temp-test-model:
    runs-on: self-hosted
    concurrency:
      group: ci-temp-test-model-${{ github.event.pull_request.number || github.ref }}
      cancel-in-progress: true
    steps:
      - name: Checkout benchmark
        uses: actions/checkout@v4

      - name: Checkout flaggems
        uses: actions/checkout@v4
        with:
          repository: FlagOpen/FlagGems
          path: flaggems

      - name: Install and run benchmark
        shell: bash
        run: |
          source flaggems/tools/run_command.sh
          run_command source "/root/miniconda3/etc/profile.d/conda.sh"
          run_command conda init bash
          run_command conda activate new-torchbenchmark
          run_command pip install -e ./flaggems
          run_command sed -i '/self\.worker\.run("import torch")/a\        self.worker.run(\
            """\
            import flag_gems\
            flag_gems.enable(record=True, once=True, path='"'"'oplist.log'"'"')""")' torchbenchmark/__init__.py
          
          run_command python install.py models hf_GPT2 
          
          run_command python run_benchmark.py test_bench --accuracy --device cuda --test eval --output output.json --models hf_GPT2

          run_command sed -i '/self\.worker\.run($/,/^[[:space:]]*flag_gems\.enable.*oplist.log.*""")/d' torchbenchmark/__init__.py